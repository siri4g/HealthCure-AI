{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOGnXoN7E2Cc",
        "outputId": "36a2c26d-c401-487a-edd8-8e8dd393c240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "WARNING: Running on CPU (training will be slow)\n",
            "Environment check completed.\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# CELL 1: ENVIRONMENT & GPU CHECK\n",
        "# ==============================\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Environment info\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"WARNING: Running on CPU (training will be slow)\")\n",
        "\n",
        "print(\"Environment check completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 2: MOUNT GOOGLE DRIVE\n",
        "# ============================\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-_p3oTOGO2d",
        "outputId": "bc836000-3053-4044-a380-72fa249442df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 3: DATASET EXTRACTION & VERIFICATION\n",
        "# =========================================\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/alzheimersdataset.zip\"\n",
        "extract_path = \"/content/alzheimersdataset\"\n",
        "\n",
        "# Extract only once\n",
        "if not os.path.exists(extract_path):\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\" Dataset extracted successfully.\")\n",
        "else:\n",
        "    print(\" Dataset already extracted.\")\n",
        "\n",
        "# Inspect extracted contents\n",
        "print(\"\\n Contents inside extracted folder:\")\n",
        "items = os.listdir(extract_path)\n",
        "print(items)\n",
        "\n",
        "# List class folders only\n",
        "print(\"\\n Detected class folders:\")\n",
        "for item in items:\n",
        "    if os.path.isdir(os.path.join(extract_path, item)):\n",
        "        print(\" -\", item)\n",
        "\n",
        "print(\"\\n Dataset structure check completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kod2LtZcGeQR",
        "outputId": "4db8ae22-2f9c-431c-b014-39b431894d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset extracted successfully.\n",
            "\n",
            " Contents inside extracted folder:\n",
            "['Data']\n",
            "\n",
            " Detected class folders:\n",
            " - Data\n",
            "\n",
            " Dataset structure check completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 4: VERIFY CLASS FOLDERS INSIDE /Data\n",
        "# =========================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Correct base path (important!)\n",
        "data_path = \"/content/alzheimersdataset/Data\"\n",
        "\n",
        "print(\" Contents inside Data folder:\")\n",
        "print(os.listdir(data_path))\n",
        "\n",
        "print(\"\\n Class folders detected:\")\n",
        "for item in os.listdir(data_path):\n",
        "    if os.path.isdir(os.path.join(data_path, item)):\n",
        "        print(\" -\", item)\n",
        "\n",
        "print(\"\\n Data folder structure verified.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJqqktfHHVhc",
        "outputId": "c626d423-717d-442b-e041-b509d8281d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Contents inside Data folder:\n",
            "['Moderate Dementia', 'Non Demented', 'Very mild Dementia', 'Mild Dementia']\n",
            "\n",
            " Class folders detected:\n",
            " - Moderate Dementia\n",
            " - Non Demented\n",
            " - Very mild Dementia\n",
            " - Mild Dementia\n",
            "\n",
            " Data folder structure verified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 5: STANDARDIZE CLASS FOLDER NAMES\n",
        "# =========================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Original data location\n",
        "src_base = \"/content/alzheimersdataset/Data\"\n",
        "\n",
        "# Clean working directory\n",
        "clean_base = \"/content/oasis_clean\"\n",
        "os.makedirs(clean_base, exist_ok=True)\n",
        "\n",
        "# Mapping original ‚Üí clean names\n",
        "class_map = {\n",
        "    \"Non Demented\": \"NonDemented\",\n",
        "    \"Very mild Dementia\": \"VeryMildDemented\",\n",
        "    \"Mild Dementia\": \"MildDemented\",\n",
        "    \"Moderate Dementia\": \"ModerateDemented\"\n",
        "}\n",
        "\n",
        "for src_name, clean_name in class_map.items():\n",
        "    src_path = os.path.join(src_base, src_name)\n",
        "    dst_path = os.path.join(clean_base, clean_name)\n",
        "\n",
        "    if not os.path.exists(dst_path):\n",
        "        shutil.copytree(src_path, dst_path)\n",
        "        print(f\" Copied {src_name} ‚Üí {clean_name}\")\n",
        "    else:\n",
        "        print(f\" {clean_name} already exists\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\n Clean dataset folders:\")\n",
        "print(os.listdir(clean_base))\n",
        "\n",
        "print(\"\\n Class name standardization complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaxzayoaHe0z",
        "outputId": "5397f469-acc6-4d11-8e30-5f035c3598e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Copied Non Demented ‚Üí NonDemented\n",
            " Copied Very mild Dementia ‚Üí VeryMildDemented\n",
            " Copied Mild Dementia ‚Üí MildDemented\n",
            " Copied Moderate Dementia ‚Üí ModerateDemented\n",
            "\n",
            " Clean dataset folders:\n",
            "['NonDemented', 'MildDemented', 'ModerateDemented', 'VeryMildDemented']\n",
            "\n",
            " Class name standardization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 6: CREATE TRAIN / VALIDATION SPLIT\n",
        "# =========================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Source (cleaned classes)\n",
        "source_dir = \"/content/oasis_clean\"\n",
        "\n",
        "# Output split directory\n",
        "split_dir = \"/content/oasis_split\"\n",
        "train_dir = os.path.join(split_dir, \"train\")\n",
        "val_dir = os.path.join(split_dir, \"val\")\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Split config\n",
        "VAL_RATIO = 0.2\n",
        "SEED = 42\n",
        "\n",
        "classes = sorted(os.listdir(source_dir))\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(source_dir, cls)\n",
        "    images = sorted(os.listdir(cls_path))\n",
        "\n",
        "    train_imgs, val_imgs = train_test_split(\n",
        "        images,\n",
        "        test_size=VAL_RATIO,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
        "\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(\n",
        "            os.path.join(cls_path, img),\n",
        "            os.path.join(train_dir, cls, img)\n",
        "        )\n",
        "\n",
        "    for img in val_imgs:\n",
        "        shutil.copy(\n",
        "            os.path.join(cls_path, img),\n",
        "            os.path.join(val_dir, cls, img)\n",
        "        )\n",
        "\n",
        "print(\" Train/Validation split created.\\n\")\n",
        "\n",
        "# Sanity check: class counts\n",
        "for split, path in [(\"TRAIN\", train_dir), (\"VAL\", val_dir)]:\n",
        "    print(f\"--> {split} distribution:\")\n",
        "    for cls in classes:\n",
        "        count = len(os.listdir(os.path.join(path, cls)))\n",
        "        print(f\"  {cls}: {count}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icCWHkpxHqnf",
        "outputId": "2de92782-5c7b-4c6c-a897-6b653f3f435c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train/Validation split created.\n",
            "\n",
            "--> TRAIN distribution:\n",
            "  MildDemented: 4001\n",
            "  ModerateDemented: 390\n",
            "  NonDemented: 53777\n",
            "  VeryMildDemented: 10980\n",
            "\n",
            "--> VAL distribution:\n",
            "  MildDemented: 1001\n",
            "  ModerateDemented: 98\n",
            "  NonDemented: 13445\n",
            "  VeryMildDemented: 2745\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 7: TRANSFORMS + BALANCED DATALOADERS\n",
        "# =========================================\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/oasis_split\"\n",
        "\n",
        "# MRI-safe transforms (NO ColorJitter)\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomAffine(\n",
        "            degrees=15,\n",
        "            translate=(0.05, 0.05),\n",
        "            scale=(0.95, 1.05)\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# ImageFolder datasets\n",
        "image_datasets = {\n",
        "    x: datasets.ImageFolder(\n",
        "        os.path.join(DATA_DIR, x),\n",
        "        transform=data_transforms[x]\n",
        "    )\n",
        "    for x in [\"train\", \"val\"]\n",
        "}\n",
        "\n",
        "class_names = image_datasets[\"train\"].classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\" Classes:\", class_names)\n",
        "\n",
        "# --------- CLASS BALANCING (CRITICAL) ---------\n",
        "targets = image_datasets[\"train\"].targets\n",
        "class_counts = np.bincount(targets)\n",
        "\n",
        "print(\"\\n Raw class counts:\")\n",
        "for cls, cnt in zip(class_names, class_counts):\n",
        "    print(f\"{cls}: {cnt}\")\n",
        "\n",
        "class_weights = 1.0 / class_counts\n",
        "sample_weights = [class_weights[t] for t in targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "dataloaders = {\n",
        "    \"train\": DataLoader(\n",
        "        image_datasets[\"train\"],\n",
        "        batch_size=32,\n",
        "        sampler=sampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    ),\n",
        "    \"val\": DataLoader(\n",
        "        image_datasets[\"val\"],\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"\\n DataLoaders created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UxbUG3VH4B4",
        "outputId": "0e989b29-a62d-4414-b0d7-f3e43eea340a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Classes: ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
            "\n",
            " Raw class counts:\n",
            "MildDemented: 4001\n",
            "ModerateDemented: 390\n",
            "NonDemented: 53777\n",
            "VeryMildDemented: 10980\n",
            "\n",
            " DataLoaders created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 8: RESNET50 MODEL[RESIDUAL NETWORK]\n",
        "# =========================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_resnet50(num_classes):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Freeze all layers first\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze deeper layers for disease-specific learning\n",
        "    for layer in [model.layer3, model.layer4]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Replace classification head\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.BatchNorm1d(num_features),\n",
        "        nn.Dropout(0.6),\n",
        "        nn.Linear(num_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "model = build_resnet50(num_classes)\n",
        "\n",
        "# Sanity check: trainable parameters\n",
        "trainable = sum(p.requires_grad for p in model.parameters())\n",
        "total = sum(1 for _ in model.parameters())\n",
        "\n",
        "print(f\"Trainable parameter tensors: {trainable}/{total}\")\n",
        "print(\"ResNet50 model ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkLpVTeHIBpL",
        "outputId": "2a4d72f9-edd8-40c4-9aae-5f202781270d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 168MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameter tensors: 91/163\n",
            "ResNet50 model ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 9: LOSS, OPTIMIZER, SCHEDULER\n",
        "# =========================================\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Compute class weights from training set\n",
        "class_counts = np.bincount(image_datasets[\"train\"].targets)\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"Class weights:\")\n",
        "for cls, w in zip(class_names, class_weights):\n",
        "    print(f\"{cls}: {w:.6f}\")\n",
        "\n",
        "# Loss: weighted CrossEntropy (critical)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer: AdamW (better generalization than Adam)\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Scheduler: cosine annealing (smooth convergence)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=25\n",
        ")\n",
        "\n",
        "print(\"\\n Loss, optimizer, and scheduler configured.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7uuGoJIJ2L",
        "outputId": "0a47fd92-2912-4c90-82e2-2d1fe046a5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights:\n",
            "MildDemented: 0.000250\n",
            "ModerateDemented: 0.002564\n",
            "NonDemented: 0.000019\n",
            "VeryMildDemented: 0.000091\n",
            "\n",
            " Loss, optimizer, and scheduler configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 10: TRAINING LOOP\n",
        "# =========================================\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=15):\n",
        "    since = time.time()\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / total_samples\n",
        "            epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "            history[f\"{phase}_loss\"].append(epoch_loss)\n",
        "            history[f\"{phase}_acc\"].append(epoch_acc.item())\n",
        "\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            # Save best model based on validation accuracy\n",
        "            if phase == \"val\" and epoch_acc > best_val_acc:\n",
        "                best_val_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"\\n‚è±Ô∏è Training completed in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
        "    print(f\"üèÜ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# üî• Start training\n",
        "model, history = train_model(\n",
        "    model,\n",
        "    dataloaders,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=15\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWDQN9lqISvO",
        "outputId": "5c354ceb-6ac5-44cf-b3df-59e86a03bbec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n",
            "------------------------------\n",
            "Train Loss: 0.0935 | Acc: 0.7617\n",
            "Val Loss: 0.7205 | Acc: 0.7264\n",
            "\n",
            "Epoch 2/15\n",
            "------------------------------\n",
            "Train Loss: 0.0312 | Acc: 0.8727\n",
            "Val Loss: 0.8002 | Acc: 0.7446\n",
            "\n",
            "Epoch 3/15\n",
            "------------------------------\n",
            "Train Loss: 0.0322 | Acc: 0.8799\n",
            "Val Loss: 0.7350 | Acc: 0.7665\n",
            "\n",
            "Epoch 4/15\n",
            "------------------------------\n",
            "Train Loss: 0.0197 | Acc: 0.9058\n",
            "Val Loss: 0.6896 | Acc: 0.7755\n",
            "\n",
            "Epoch 5/15\n",
            "------------------------------\n",
            "Train Loss: 0.0188 | Acc: 0.9104\n",
            "Val Loss: 0.5514 | Acc: 0.8219\n",
            "\n",
            "Epoch 6/15\n",
            "------------------------------\n",
            "Train Loss: 0.0170 | Acc: 0.9199\n",
            "Val Loss: 0.4691 | Acc: 0.8434\n",
            "\n",
            "Epoch 7/15\n",
            "------------------------------\n",
            "Train Loss: 0.0146 | Acc: 0.9289\n",
            "Val Loss: 0.5841 | Acc: 0.8048\n",
            "\n",
            "Epoch 8/15\n",
            "------------------------------\n",
            "Train Loss: 0.0133 | Acc: 0.9340\n",
            "Val Loss: 0.2951 | Acc: 0.8957\n",
            "\n",
            "Epoch 9/15\n",
            "------------------------------\n",
            "Train Loss: 0.0091 | Acc: 0.9461\n",
            "Val Loss: 0.3611 | Acc: 0.8782\n",
            "\n",
            "Epoch 10/15\n",
            "------------------------------\n",
            "Train Loss: 0.0096 | Acc: 0.9469\n",
            "Val Loss: 0.1984 | Acc: 0.9262\n",
            "\n",
            "Epoch 11/15\n",
            "------------------------------\n",
            "Train Loss: 0.0067 | Acc: 0.9607\n",
            "Val Loss: 0.2109 | Acc: 0.9261\n",
            "\n",
            "Epoch 12/15\n",
            "------------------------------\n",
            "Train Loss: 0.0060 | Acc: 0.9640\n",
            "Val Loss: 0.1081 | Acc: 0.9603\n",
            "\n",
            "Epoch 13/15\n",
            "------------------------------\n",
            "Train Loss: 0.0049 | Acc: 0.9694\n",
            "Val Loss: 0.1119 | Acc: 0.9592\n",
            "\n",
            "Epoch 14/15\n",
            "------------------------------\n",
            "Train Loss: 0.0037 | Acc: 0.9766\n",
            "Val Loss: 0.0977 | Acc: 0.9668\n",
            "\n",
            "Epoch 15/15\n",
            "------------------------------\n",
            "Train Loss: 0.0028 | Acc: 0.9811\n",
            "Val Loss: 0.0483 | Acc: 0.9828\n",
            "\n",
            "‚è±Ô∏è Training completed in 122m 11s\n",
            "üèÜ Best Validation Accuracy: 0.9828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 11: LOSS & ACCURACY CURVES\n",
        "# =========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# -------- Loss Curve --------\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# -------- Accuracy Curve --------\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\", linewidth=2)\n",
        "plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\", linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\" Training curves plotted successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cqkFc3VjWCHt",
        "outputId": "4ca7b8e6-3952-4816-cee5-5de60c977506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3412441495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# CELL 12: CONFUSION MATRIX (VALIDATION)\n",
        "# =========================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloaders[\"val\"]:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Alzheimer‚Äôs Stage Confusion Matrix (Validation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\" Confusion matrix generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mECFaT9hWTQD",
        "outputId": "f5ec26d1-dc05-4d6c-c778-b20ea307a654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3311431870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}